# Final Project Report

**Project URL**: TODO
**Video URL**: TODO

Short (~250 words) abstract of the concrete data science problem and how the solutions addresses the problem.

The data science problem we aim to address is how human well-being in cities and states influences hate crime patterns over the years. Our approach to this problem is to address these issues and offer a broader exploration into the behaviors, demographics, and other defining features behind the causes of hate crimes. Our approach first provides a high-level overview of the prevalence of hate crimes in the U.S. from 1990 to 2020. By first pre-processing the data so that categorical variables are binarized, employed the use of dimensionality reduction and clustering techniques to best visualize the data in a lower dimensional embedded space. This aligns with our expectation that hate crimes are generally driven by many factors and our efforts will attempt to simulate that as much as possible. From here, we aim to allow users to direct their attention to any city available to explore the hate crime patterns in relation to any defining features. Users can explore the patterns in demographics and behaviors of a particular city and the prevalence of hate crimes in that city. The expectation is that the features associated with a city (i.e., education, racial diversity, income) would help explain patterns in hate crimes in that city. By exploring this relation, we hope that there will be a enough foundation to draw a robust conclusion. Through our solution, we hope that it will help give greater clarity to what influences hate crimes and inform policy actions and decisions to address these crimes in cities and states. Further study generated from the conclusions of these results would be to further explore the most defining features and focus research on how these features contribute to biases and crime. 

## Introduction

In much of American history, there exists a pervasive and persistent tension between people of racial and ethnic groups. This tension has evolved into hate crimes that range from intimidation, verbal assault, assault, and murder in recent years. Despite the best efforts of educators, policymakers, and community organizers, tensions seem to increase along with corresponding crime. As a group, we were disheartened by the rise in recent hate crimes committed against the Asian American community. With many of us having loved ones who live in the U.S., we wanted to understand why these incidents happen so that we can develop areas for improvement. 

Crime is generally well-studied in academic and public spaces. Crime data is readily accessible through local and federal law enforcement agencies such as the FBI. As such, hate-based crime can be readily viewed through online tools created by these agencies and readily downloaded to be manipulated for further analysis. However, upon further interaction with these tools, it can be seen that these tools are fairly limited in their purpose and that a shallow conclusion could be drawn from these statistics. 

For the public, current visualizations and tools for exploring hate crime incidents are often compiled into dashboards or graphs showcasing simple statistics of a particular crime. While this allows for creating a concise and pertinent data-driven story, it is a straightforward analysis of the data that may not be very informative. It opens several problems regarding the flexibility of the data exploration with other correlated features and the robustness of any conclusions made from the data. In fact, the graphs, charts, and even tables may not reflect the particular purpose that the user has with the data. When our team examined news articles behind recent hate crime incidents and reflected upon our own experiences, we collectively felt that it wasn't sufficient to conclude that people needed to discriminate less and have less bias. A common explanation is that education is necessary to correct bias and that early-elementary education can be leveraged to reduce future criminal activity [1]. While we wholeheartedly agree that this is an essential component of addressing this issue, hate crimes are still complex occurrences of many factors. Therefore, a singular, "one-fits-all" approach would not address hate crime patterns sufficiently. 

We have seen that the causes behind crimes and specifically hate crimes, are expected to be highly multi-dimensional and, thus, impossible to assign a single reason to an incident. Additionally, many studies view the prevalence of hate crimes as attributed to trends or spikes. For example, the 2020 COVID pandemic saw a spike in Anti-Asian hate crimes due to negative stereotyping of Asian-Americans concerning COVID-19. Mainly, we've seen these crimes occur in highly dense, urban metropolitan cities. We'd expect that education is relatively among the populace in these cities, yet hate crimes still happen more frequently in these areas. This leads us to assert that additional factors influence hate crimes in these cities. We are doubtful that these trends or spikes can be attributed to a particular reason and that perhaps, many other factors have been at work before the global pandemic. 

 A robust analysis of the causes behind hate crimes will need the flexibility to explore and analyze the data with additional or related datasets regarding human well-being that can help inform us about hate crime patterns. A part of this analysis would be to explore coinciding trends or spikes in demographics along with those in hate crimes. We intend to solve this problem by creating a robust application that clearly considers various factors surrounding hate crimes to better understand why these crimes happen in the first place. 



## Related Work

## Methods
One set of techniques used on the FBI Hate Crimes dataset is a dimensionality reduction technique called UMAP and a clustering technique called DBSCAN. With these two techniques, I was able to create a series of charts that clustered on the data points and labeled them accordingly. The idea is to highlight similar pairwise points together so that by grouping them, we can determine what their similar attributes are. With dimensionality reduction, we can embed the data from a higher dimension into a lower-dimensional space that can still retain information or structure in the data. We knew that we didn't want to use Principal Component Analysis (PCA) since that technique uses a linear projection of the data and tends to overemphasize the global structure of the data rather than preserve local structures. In fact, tests using PCA yielded groupings that were rather unsatisfactory to how we expected to behave. The next step was to see if using a probabilistic interpretation of pairwise similarities would fare better for our analysis. It is with this we could use t-SNE to construct our embeddings. However, with t-SNE, our resulting 2-dimensional embeddings resulted in clusters that seemed too apparent as the pairwise similarities between points seemed to be magnified. Upon further analysis, it appeared that t-SNE was preserving much of the local structure in the data. We then also embedded our data using DensMAP, a variation of UMAP. Compared to t-SNE, DensMAP seemed to work better in terms of preserving a balance of local and global structure. Despite DensMAP performing relatively slower than t-SNE, we felt that DensMAP provided a more accurate representation of the global structure of the data. From there, we decided to use a nonparametric clustering algorithm like DBSCAN as it allows us to focus on densities instead of assuming that the data follows a Gaussian distribution. The densities computed allow us to create multi-labeled predictions for which clusters the points fall into. This allowed us to extend the functionality of our visualization, as we can now iteratively add more "information" that can help explain the distribution of our data. 

## Results

## Discussion

## Future Work
